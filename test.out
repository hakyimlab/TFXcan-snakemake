Building DAG of jobs...
Job stats:
job                                count    min threads    max threads
-------------------------------  -------  -------------  -------------
aggregate_predictions                  1              1              1
all                                    1              1              1
calculate_enpact_scores                1              1              1
collect_finemapping_results            1              1              1
create_enformer_configuration          1              1              1
predict_with_enformer                  1              1              1
prepare_files_for_predictDB            1              1              1
run_susie_on_summary_statistics        1              1              1
total                                  8              1              1


[Wed Feb 14 19:23:46 2024]
Job 2: working on phenotype=asthma_children
Reason: Missing output files: data/finemapping/asthma_children


        module load parallel;
        printf "%s\n"  | parallel -j 12 "/beagle3/haky/users/shared_software/TFXcan-pipeline-tools/bin/Rscript workflow/src/run_susie_on_summary_statistics.R --chromosome {} --sumstats data/processed_sumstats/asthma_children/chr{}.sumstats.txt.gz --LDBlocks_info /project2/haky/Data/LD_blocks/hg38/EUR/hg38_fourier_ls-all.bed --output_folder data/finemapping/asthma_children --phenotype asthma_children --pip_threshold 0.5 --diagnostics_file data/diagnostics/asthma_children.chr{}.susie_diagnostics.summary"
        

[Wed Feb 14 19:23:46 2024]
Job 3: working on phenotype=asthma_children
Reason: Missing output files: data/collection/asthma_children/asthma_children.filteredGWAS.txt.gz, data/collection/asthma_children/asthma_children.EnformerLoci.txt; Input files updated by another job: data/finemapping/asthma_children


        /beagle3/haky/users/shared_software/TFXcan-pipeline-tools/bin/Rscript workflow/src/collect_finemapping_results.R --finemapping_dir data/finemapping/asthma_children --phenotype asthma_children --filtered_sumstats data/collection/asthma_children/asthma_children.filteredGWAS.txt.gz --enformer_loci data/collection/asthma_children/asthma_children.EnformerLoci.txt
        

[Wed Feb 14 19:23:46 2024]
Job 4: working on phenotype=asthma_children
Reason: Missing output files: data/enformer_parameters/enformer_parameters_Asthma_GWAS_asthma_children.json; Input files updated by another job: data/collection/asthma_children/asthma_children.EnformerLoci.txt

[Wed Feb 14 19:23:46 2024]
Job 5: working on asthma_children
Reason: Missing output files: data/enformer_parameters/aggregation_config_Asthma_GWAS_asthma_children.json; Input files updated by another job: data/enformer_parameters/enformer_parameters_Asthma_GWAS_asthma_children.json


        python3 /beagle3/haky/users/shared_pipelines/enformer_pipeline_aggregate/scripts/enformer_predict.py --parameters data/enformer_parameters/enformer_parameters_Asthma_GWAS_asthma_children.json
        

[Wed Feb 14 19:23:46 2024]
Job 6: working on phenotype=asthma_children
Reason: Missing output files: data/checkpoints/asthma_children.checkpoint, data/aggregated_predictions/asthma_children; Input files updated by another job: data/enformer_parameters/aggregation_config_Asthma_GWAS_asthma_children.json; Code has changed since last execution; Params have changed since last execution

[Wed Feb 14 19:23:46 2024]
Job 7: working on asthma_children
Reason: Missing output files: data/enpact_predictions/asthma_children; Input files updated by another job: data/checkpoints/asthma_children.checkpoint, data/aggregated_predictions/asthma_children


        module load parallel;
        mkdir -p data/enpact_predictions/asthma_children;
        printf "%s\n"  | parallel -j 5 '/beagle3/haky/users/shared_software/TFXcan-pipeline-tools/bin/Rscript workflow/src/calculate_enpact_scores.R --input_file data/aggregated_predictions/asthma_children/{}_aggByCollect_asthma_children.csv.gz --output_file data/enpact_predictions/asthma_children/{}.asthma_children.aggByCollect.2024-01-31.csv.gz --enpact_models_directory /project2/haky/temi/projects/TFPred-snakemake/output/models --filters_date 2023-12-01 --filters_type logistic'
        

[Wed Feb 14 19:23:46 2024]
rule prepare_files_for_predictDB:
    input: data/enpact_predictions/asthma_children
    output: data/predictdb/asthma_children/asthma_children.enpact_scores.txt, data/predictdb/asthma_children/asthma_children.tf_tissue_annot.txt
    jobid: 8
    reason: Missing output files: data/predictdb/asthma_children/asthma_children.enpact_scores.txt, data/predictdb/asthma_children/asthma_children.tf_tissue_annot.txt; Input files updated by another job: data/enpact_predictions/asthma_children
    wildcards: phenotype=asthma_children
    resources: mem_mb=<TBD>, disk_mb=<TBD>, tmpdir=<TBD>, partition=caslake, time=02:00:00, account=pi-haky, nodes=1, gpu=0, mem_cpu=4, cpu_task=8

/beagle3/haky/users/shared_software/TFXcan-pipeline-tools/bin/Rscript workflow/src/prepare_files_for_predictDB.R --individuals  --input_file_pattern data/enpact_predictions/asthma_children/{}.asthma_children.aggByCollect.2024-01-31.csv.gz --output_directory data/predictdb/asthma_children --filtered_GWAS_file data/collection/asthma_children/asthma_children.filteredGWAS.txt.gz --phenotype asthma_children

[Wed Feb 14 19:23:46 2024]
localrule all:
    input: data/sumstats/asthma_children.liftover.logistic.assoc.tsv.gz, data/processed_sumstats/asthma_children, data/finemapping/asthma_children, data/collection/asthma_children/asthma_children.filteredGWAS.txt.gz, data/collection/asthma_children/asthma_children.EnformerLoci.txt, data/enformer_parameters/enformer_parameters_Asthma_GWAS_asthma_children.json, data/enformer_parameters/aggregation_config_Asthma_GWAS_asthma_children.json, data/checkpoints/asthma_children.checkpoint, data/aggregated_predictions/asthma_children, data/enpact_predictions/asthma_children, data/predictdb/asthma_children/asthma_children.enpact_scores.txt, data/predictdb/asthma_children/asthma_children.tf_tissue_annot.txt
    jobid: 0
    reason: Input files updated by another job: data/enformer_parameters/enformer_parameters_Asthma_GWAS_asthma_children.json, data/collection/asthma_children/asthma_children.EnformerLoci.txt, data/collection/asthma_children/asthma_children.filteredGWAS.txt.gz, data/enpact_predictions/asthma_children, data/enformer_parameters/aggregation_config_Asthma_GWAS_asthma_children.json, data/finemapping/asthma_children, data/aggregated_predictions/asthma_children, data/checkpoints/asthma_children.checkpoint, data/predictdb/asthma_children/asthma_children.enpact_scores.txt, data/predictdb/asthma_children/asthma_children.tf_tissue_annot.txt
    resources: mem_mb=<TBD>, disk_mb=<TBD>, tmpdir=/scratch/midway3/temi, partition=caslake, time=02:00:00, account=pi-haky, nodes=1, gpu=0, mem_cpu=4, cpu_task=8

Job stats:
job                                count    min threads    max threads
-------------------------------  -------  -------------  -------------
aggregate_predictions                  1              1              1
all                                    1              1              1
calculate_enpact_scores                1              1              1
collect_finemapping_results            1              1              1
create_enformer_configuration          1              1              1
predict_with_enformer                  1              1              1
prepare_files_for_predictDB            1              1              1
run_susie_on_summary_statistics        1              1              1
total                                  8              1              1

Reasons:
    (check individual jobs above for details)
    code has changed since last execution:
        aggregate_predictions
    input files updated by another job:
        aggregate_predictions, all, calculate_enpact_scores, collect_finemapping_results, create_enformer_configuration, predict_with_enformer, prepare_files_for_predictDB
    missing output files:
        aggregate_predictions, calculate_enpact_scores, collect_finemapping_results, create_enformer_configuration, predict_with_enformer, prepare_files_for_predictDB, run_susie_on_summary_statistics
    params have changed since last execution:
        aggregate_predictions
Some jobs were triggered by provenance information, see 'reason' section in the rule displays above.
If you prefer that only modification time is used to determine whether a job shall be executed, use the command line option '--rerun-triggers mtime' (also see --help).
If you are sure that a change for a certain output file (say, <outfile>) won't change the result (e.g. because you just changed the formatting of a script or environment definition), you can also wipe its metadata to skip such a trigger via 'snakemake --cleanup-metadata <outfile>'. 
Rules with provenance triggered jobs: aggregate_predictions


This was a dry-run (flag -n). The order of jobs does not reflect the order of execution.
